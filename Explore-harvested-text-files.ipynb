{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore harvested text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import altair as alt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords += [\"tho\", \"tbe\"]\n",
    "\n",
    "# Are you using Jupyter Lab?\n",
    "# If so either don't run this cell or comment out the line below\n",
    "\n",
    "# alt.renderers.enable('notebook')\n",
    "\n",
    "# If you forget, run this cell, and then get strange warnings when you make a chart,\n",
    "# uncomment the following line and run this cell to reset the chart renderer\n",
    "\n",
    "# alt.renderers.enable('default')\n",
    "\n",
    "# alt.data_transformers.enable('json')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load variables from the .env file if it exists\n",
    "# Use %%capture to suppress messages\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Import a harvest zip file you've created previously\n",
    "# First upload the zip file to the data directory, then run this cell\n",
    "\n",
    "for zipped in sorted(Path(\"data\").glob(\"*.zip\")):\n",
    "    print(f\"Unzipping {zipped}...\")\n",
    "    with zipfile.ZipFile(zipped, \"r\") as zip_file:\n",
    "        zip_file.extractall(Path(f\"data/{zipped.stem}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_harvest():\n",
    "    \"\"\"\n",
    "    Get the timestamp of the most recent harvest.\n",
    "    \"\"\"\n",
    "    harvests = sorted(\n",
    "        [d for d in Path(\"data\").iterdir() if d.is_dir() and not d.name.startswith(\".\")]\n",
    "    )\n",
    "    try:\n",
    "        harvest = harvests[-1]\n",
    "    except IndexError:\n",
    "        print(\"No harvests!\")\n",
    "        harvest = None\n",
    "    return harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(harvest):\n",
    "    docs_path = get_docs_path(harvest)\n",
    "    for p in docs_path:\n",
    "        yield p.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "\n",
    "def get_docs_path(harvest):\n",
    "    path = Path(harvest, \"text\")\n",
    "    docs_path = [p for p in sorted(path.glob(\"*.txt\"))]\n",
    "    return docs_path\n",
    "\n",
    "\n",
    "def get_file_names(harvest):\n",
    "    return [p.stem for p in get_docs_path(harvest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In testing environment, open a test harvest\n",
    "if os.getenv(\"GW_STATUS\") == \"dev\":\n",
    "    harvest = Path(\"data\", \"1655952487\")\n",
    "# Otherwise open most recent harvest\n",
    "# Supply a harvest directory name to open a specific harvest\n",
    "else:\n",
    "    harvest = get_latest_harvest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=frozenset(stopwords), max_features=10000, ngram_range=(1, 1)\n",
    ")\n",
    "# preprocessor = lambda x: re.sub(r'(\\d[\\d\\.])+', 'NUM', x.lower())\n",
    "X_freq = np.asarray(vectorizer.fit_transform(get_docs(harvest)).todense())\n",
    "df_freq = pd.DataFrame(\n",
    "    X_freq, columns=vectorizer.get_feature_names_out(), index=get_file_names(harvest)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq.sum().nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq.unstack().to_frame().reset_index().dropna(axis=0, subset=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The number of words you want to show\n",
    "num_words = 10\n",
    "top_words = pd.DataFrame(\n",
    "    {\n",
    "        n: df_freq.T[col].nlargest(num_words).index.tolist()\n",
    "        for n, col in enumerate(df_freq.T)\n",
    "    }\n",
    ").T\n",
    "top_words.index = get_file_names(harvest)\n",
    "top_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a 'year' column to the dataframe\n",
    "\n",
    "Each file name includes the date on which the article was published. For example, `18601224-13-5696044` was published on 24 December 1860. We can easily extract the year by just slicing the first four characters off the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq[\"article_year\"] = df_freq.index.str.slice(0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and sum the word counts\n",
    "year_groups = df_freq.groupby(by=\"article_year\")\n",
    "year_group_totals = year_groups.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape so that we have columns for year, word, and count\n",
    "words_by_year = year_group_totals.unstack().to_frame().reset_index()\n",
    "words_by_year.columns = [\"word\", \"year\", \"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_by_year = (\n",
    "    words_by_year.sort_values(\"count\", ascending=False)\n",
    "    .groupby(by=[\"year\"])\n",
    "    .head(10)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_by_year[\"word\"].value_counts()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise top ten words per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(top_words_by_year).mark_bar().encode(\n",
    "    y=alt.Y(\"word:N\", sort=\"-x\"), x=\"count:Q\", facet=alt.Facet(\"year\", columns=4)\n",
    ").properties(width=120, height=120).resolve_scale(x=\"independent\", y=\"independent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise word frequencies over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a faceted chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    words_by_year.loc[words_by_year[\"word\"].isin([\"storm\", \"cyclone\", \"snow\"])]\n",
    ").mark_line().encode(\n",
    "    x=alt.X(\"year:Q\", axis=alt.Axis(format=\"c\", title=\"Year\")),\n",
    "    y=\"count:Q\",\n",
    "    color=\"word:N\",\n",
    "    facet=alt.Facet(\"word:N\", columns=1),\n",
    ").properties(\n",
    "    width=700, height=100\n",
    ").resolve_scale(\n",
    "    y=\"independent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org) ([@wragge](https://twitter.com/wragge)) for the [GLAM Workbench](https://github.com/glam-workbench/).  \n",
    "Support this project by [becoming a GitHub sponsor](https://github.com/sponsors/wragge?o=esb).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
